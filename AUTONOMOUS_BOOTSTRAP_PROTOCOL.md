# Autonomous Bootstrap Protocol

**A meta-level protocol for Kent + Claude Code to implement kgents.**

This document is self-referential: Claude Code reads it to guide implementation, and the implementation validates the protocol.

---

## Core Principle: Autopoiesis First

**Use kgents to build kgents from the FIRST line of code.**

```python
# Step 0: Before writing ANY implementation
from runtime import ClaudeCLIRuntime
from agents.k import kgent, DialogueMode, DialogueInput
from agents.a import creativity_coach, CreativityInput, CreativityMode

runtime = ClaudeCLIRuntime()

# Use K-gent for naming decisions
name = await runtime.execute(kgent(), DialogueInput(
    message="What should I name this component?",
    mode=DialogueMode.ADVISE
))

# Use CreativityCoach for design exploration
design = await runtime.execute(creativity_coach(), CreativityInput(
    seed="the problem I'm solving",
    mode=CreativityMode.EXPAND
))
```

### Complete Workflow Example: Agent-Driven Design

**Scenario**: Implementing a new "MemoryAgent" that maintains conversation history.

**Session demonstrates >50% autopoiesis:**

```python
from runtime import ClaudeCLIRuntime
from agents.a import creativity_coach, CreativityMode, CreativityInput
from agents.k import kgent, DialogueMode, DialogueInput
from agents.b import hypothesis_engine, HypothesisInput
from bootstrap import judge, contradict, sublate

runtime = ClaudeCLIRuntime()

# 1. EXPLORATION (CreativityCoach)
print("=== Phase 1: Design Exploration ===")
exploration = await runtime.execute(
    creativity_coach(),
    CreativityInput(
        seed="Agent that maintains conversation history for context",
        mode=CreativityMode.EXPAND
    )
)
# Output: 5 design approaches (in-memory list, SQLite, file-based, Redis, etc.)

# 2. VALIDATION (HypothesisEngine)
print("=== Phase 2: Architecture Validation ===")
validation = await runtime.execute(
    hypothesis_engine(),
    HypothesisInput(
        observations=[
            "Need persistence across sessions",
            "Must be fast for real-time chat",
            "Should handle 1000+ messages"
        ],
        domain="software architecture",
        question="Which storage approach is most suitable?"
    )
)
# Output: Hypotheses ranking approaches by criteria

# 3. NAMING (K-gent)
print("=== Phase 3: Component Naming ===")
naming = await runtime.execute(
    kgent(),
    DialogueInput(
        message="I'm implementing a conversation history agent. Should I call it MemoryAgent, HistoryAgent, or ContextAgent?",
        mode=DialogueMode.ADVISE
    )
)
# Output: "HistoryAgent - 'memory' is overloaded, 'history' is precise"

# 4. IMPLEMENTATION (Manual, guided by above)
print("=== Phase 4: Implementation ===")
# Write HistoryAgent based on chosen architecture
# (This part is mechanical translation from design)

# 5. JUDGMENT (Judge)
print("=== Phase 5: Quality Check ===")
verdict = await judge.invoke(JudgeInput(
    agent_spec=history_agent_code,
    principles=PRINCIPLES
))
if verdict.type == VerdictType.REVISE:
    print(f"Revisions needed: {verdict.revisions}")
    # Apply revisions, re-run judge (Fix pattern)

# 6. CONSISTENCY CHECK (Contradict)
print("=== Phase 6: Consistency Verification ===")
tension = await contradict.invoke((spec_expectations, actual_implementation))
if tension:
    print(f"Tension detected: {tension.description}")
    resolution = await sublate.invoke(tension)
    print(f"Resolution: {resolution.explanation}")

# Autopoiesis score: 5/6 phases used agents (83%)
```

**Key points demonstrated**:
1. **Agents used continuously** - not just initial setup
2. **Design decisions delegated** to appropriate agents
3. **Human judgment preserved** - final implementation choice remains with Kent
4. **Iterative refinement** - Judge â†’ revise â†’ Judge (Fix pattern)
5. **Explicit verification** - Contradict/Sublate for consistency

**This session achieves >50% autopoiesis**: Design, validation, naming, judgment, and consistency checking all agent-driven.

**Autopoiesis Score** = (lines generated by kgents agents) / (total lines)
- Target: >50%
- Measure this. Track it. Improve it.

---

## Ground: The Bootstrap Kernel

```
impl/claude/
â”œâ”€â”€ bootstrap/           # 7 irreducible agents
â”‚   â”œâ”€â”€ id.py           # Identity: A â†’ A
â”‚   â”œâ”€â”€ compose.py      # Composition: (Agent, Agent) â†’ Agent
â”‚   â”œâ”€â”€ judge.py        # Judgment: (Agent, Principles) â†’ Verdict
â”‚   â”œâ”€â”€ ground.py       # Grounding: Void â†’ Facts
â”‚   â”œâ”€â”€ contradict.py   # Contradiction: (A, B) â†’ Tension | None
â”‚   â”œâ”€â”€ sublate.py      # Synthesis: Tension â†’ Synthesis | Hold
â”‚   â”œâ”€â”€ fix.py          # Fixed-point: (A â†’ A) â†’ A
â”‚   â””â”€â”€ types.py        # Agent[A, B] base class
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ a/              # Abstract + Art (AbstractSkeleton, CreativityCoach)
â”‚   â”œâ”€â”€ b/              # Bio/Scientific (HypothesisEngine, Robin)
â”‚   â”œâ”€â”€ c/              # Category theory (Maybe, Either, parallel, race, branch, switch)
â”‚   â”œâ”€â”€ h/              # Dialectics (Hegel, Jung, Lacan)
â”‚   â””â”€â”€ k/              # Persona (KgentAgent, PersonaQueryAgent, EvolutionAgent)
â””â”€â”€ runtime/            # LLM execution (ClaudeCLIRuntime, ClaudeRuntime, OpenRouterRuntime)
```

---

## Bootstrap Agent Decision Matrix

**When to invoke which bootstrap agent?** Use this table for real-time decision-making:

| Situation | Agent(s) | Mode/Pattern | Example |
|-----------|----------|--------------|---------|
| **Naming a component** | K-gent | `DialogueMode.ADVISE` | "What should I call this class?" |
| **Design uncertainty** | CreativityCoach â†’ HypothesisEngine â†’ Judge | `EXPAND` â†’ validate â†’ evaluate | Exploring architecture options |
| **Configuration conflicts** | Contradict â†’ Sublate | Detect tension â†’ synthesize or hold | Config defaults vs user overrides |
| **Need to retry/iterate** | Fix | Define transform + equality_check | Polling, convergence, retry logic |
| **Two approaches contradict** | Contradict â†’ Sublate | All modes (LOGICAL, PRAGMATIC, etc.) | "Spec says X, impl does Y" |
| **Starting from scratch** | Ground | Load persona/world/history | Initial project setup, persona extraction |
| **Building pipelines** | Compose (`>>`) | Sequential composition | `validate >> transform >> persist` |
| **Quality check** | Judge | Seven mini-judges composed | Review against 7 principles |
| **Identity/passthrough** | Id | Composition unit | Testing, default case, category laws |

### Decision Tree for Common Scenarios

**Scenario: "I'm stuck on a design decision"**
```
1. Use CreativityCoach (EXPAND mode) to explore options
2. Use K-gent (CHALLENGE mode) to check against persona preferences
3. Use HypothesisEngine to validate architectural soundness
4. Use Judge to evaluate against principles
```

**Scenario: "Something feels wrong but I can't pinpoint it"**
```
1. Use Contradict to compare current vs expected
2. If tension found â†’ Sublate to synthesize or consciously hold
3. If no tension but still feels off â†’ use K-gent (REFLECT mode)
```

**Scenario: "I need this to work reliably"**
```
1. Use Fix for retry/convergence pattern
2. Use Judge to ensure quality
3. Use Contradict to verify consistency with spec
```

**Scenario: "Starting a new implementation"**
```
1. Ground: Load persona and context
2. CreativityCoach: Explore design space
3. K-gent: Get naming/style guidance
4. Compose: Build pipeline from small agents
5. Judge: Validate against principles
6. Fix: Iterate until stable
```

---

## The Protocol

### Principle: Heterarchical Collaboration

Neither Kent nor Claude Code is "in charge." The collaboration is heterarchical:
- **Kent provides Ground**: preferences, judgment calls, domain knowledge
- **Claude Code provides Fix**: iteration, exploration, mechanical translation
- **Both apply Judge**: reviewing against the 7 principles

### Principle: Spec Before Impl

1. Write specification in `spec/{letter}-gents/`
2. Read spec completely before implementing
3. Implement as mechanical translation
4. Validate: `Contradict(impl, spec) = None`

#### Resolving the Spec-First / Agents-First Tension

**The tension**: Protocol says "Spec Before Impl" but also says "USE kgents agents FIRST."

**Synthesis**:

1. **For new agent families (D-gents, E-gents, etc.)**:
   - Write spec first (Ground: define what you're building)
   - Use agents to implement (autopoiesis: build with kgents)
   - Pattern: `spec/*.md â†’ agents.{letter}/*.py`

2. **For bootstrap agents themselves**:
   - They ARE the spec (self-describing)
   - Implementation and specification co-evolve
   - Pattern: `spec/bootstrap.md â‰… bootstrap/*.py`

3. **For design exploration** (when spec doesn't exist yet):
   - Use agents to GENERATE spec candidates
   - CreativityCoach explores design space
   - HypothesisEngine validates architectures
   - K-gent reviews against persona
   - THEN crystallize into spec
   - Pattern: `agents â†’ spec.md â†’ refined implementation`

See `docs/BOOTSTRAP_PROMPT.md` for detailed implementation guidance for each agent.

**Decision rule**:
```
IF spec exists THEN
    implement from spec (using agents to help)
ELSE IF exploring new territory THEN
    use agents to generate spec candidates
ELSE IF regenerating bootstrap THEN
    spec and impl are isomorphic
```

**Example 1: Implementing C-gents** (spec exists)
```python
# 1. Read spec/c-gents/composition.md
# 2. Use CreativityCoach to explore implementation approaches
# 3. Implement mechanically from spec
# 4. Contradict(impl, spec) should be None
```

**Example 2: Designing F-gents** (spec doesn't exist)
```python
# 1. Use CreativityCoach to explore "what F-gents could be"
# 2. Use HypothesisEngine to validate ideas
# 3. Use K-gent to align with persona
# 4. Write spec/f-gents/*.md
# 5. Implement from spec
```

**Example 3: Regenerating Judge** (spec IS impl)
```python
# 1. Read spec/bootstrap.md (defines Judge abstractly)
# 2. Read bootstrap/judge.py (defines Judge concretely)
# 3. Use agents to regenerate, verify isomorphism
# 4. Contradict(original, regenerated) should find no critical tensions
```

This resolution preserves both principles:
- Spec-first for new work (grounding)
- Agents-first for process (autopoiesis)

---

## Required Patterns

### Pattern: Events are Sublate Traces

State changes MUST emit events:
```python
# WRONG
del self._sessions[id]
return True

# RIGHT
del self._sessions[id]
await self._emit_event(SessionRemoved(session_id=id))
return True
```

### Pattern: Ground Includes Environment

Shell, platform, package structure are Groundâ€”explicit, not assumed:
```python
# WRONG: Assumes bash
command = "read -p 'prompt' var"

# RIGHT: POSIX is Ground
command = "printf 'prompt ' && read var"
```

**Ground checklist**:
- [ ] Shell commands are POSIX-compatible
- [ ] Package structure in `pyproject.toml` (no `sys.path` hacks)
- [ ] Config defaults merge, not overwrite
- [ ] Framework constraints documented

### Pattern: Fix Needs Memory

Fixed-point iteration carries state:
```python
# WRONG: Stateless
def detect(session) -> Result:
    return Result(confidence=0.2)

# RIGHT: Stateful
def detect(session, previous: Result) -> Result:
    if current_state == previous.state:
        return Result(confidence=min(1.0, previous.confidence + 0.2))
    return Result(state=current_state, confidence=0.2)
```

### Pattern: Sublate, Don't Overwrite

Merge configurations, preserve both sides:
```python
# WRONG
config = data.get("config", {})

# RIGHT
config = {**defaults, **data.get("config", {})}
```

### Pattern: Conflicts are Data

Never swallow exceptions silently:
```python
# WRONG
except Exception:
    pass

# RIGHT
except Exception as e:
    self.log.warning(f"Conflict: {e}")
```

---

## Common Pitfalls and How to Avoid Them

This section captures failure modes observed in practice during kgents implementation. Learn from these mistakes to maintain high autopoiesis and composability.

### Pitfall 1: Skipping Agent Invocation in Early Code

**Symptom**: Writing implementation code before consulting agents, achieving <20% autopoiesis.

**Example**:
```python
# WRONG: Jumping straight to implementation
def implement_feature():
    # Write 200 lines without using K-gent, CreativityCoach, or HypothesisEngine
    class MyNewAgent:
        ...
```

**Fix**: Use agents FIRST, especially for:
- Naming decisions â†’ K-gent (ADVISE mode)
- Design exploration â†’ CreativityCoach (EXPAND mode)
- Architecture validation â†’ HypothesisEngine
- Quality checks â†’ Judge

**Pattern**:
```python
# RIGHT: Agent-driven workflow
runtime = ClaudeCLIRuntime()

# Step 1: Explore design space
design = await runtime.execute(creativity_coach(), CreativityInput(
    seed="feature description",
    mode=CreativityMode.EXPAND
))

# Step 2: Get naming guidance
name = await runtime.execute(kgent(), DialogueInput(
    message="Which name fits better: X or Y?",
    mode=DialogueMode.ADVISE
))

# Step 3: NOW implement with informed decisions
```

### Pitfall 2: Premature Synthesis (Not Using HoldTension)

**Symptom**: Forcing synthesis when tensions should be held consciously.

**Example**:
```python
# WRONG: Always synthesizing immediately
tension = await contradict.invoke((approach_a, approach_b))
if tension:
    # Force immediate synthesis even when more exploration needed
    synthesis = await sublate.invoke(tension)
    # Might lose valuable information from tension
```

**Fix**: Use `HoldTension` when:
- Tension is productive (creative tension)
- More information needed to synthesize
- Both sides have valid perspectives

**Pattern**:
```python
# RIGHT: Conscious holding
tension = await contradict.invoke((approach_a, approach_b))
if tension:
    result = await sublate.invoke(tension)
    if isinstance(result, HoldTension):
        # Document the tension, revisit later
        self.held_tensions.append(result)
        # Continue with explicit awareness of trade-off
    else:
        # Genuine synthesis achieved
        return result.synthesis
```

### Pitfall 3: Stateless Fix Patterns

**Symptom**: Fix iterations don't converge because state isn't accumulated.

**Example**:
```python
# WRONG: No memory between iterations
def poll_status() -> Status:
    current = get_status()
    if current == "ready":
        return Status(ready=True)
    return Status(ready=False)  # Resets confidence each time

# Never convergesâ€”no accumulation
result = await fix(transform=poll_status, initial=Status(ready=False))
```

**Fix**: Carry state between iterations, accumulate confidence/evidence.

**Pattern**:
```python
# RIGHT: Stateful accumulation
def poll_with_memory(previous: Status) -> Status:
    current = get_status()
    if current == "ready":
        # Accumulate confidence
        return Status(
            ready=True,
            confidence=min(1.0, previous.confidence + 0.2)
        )
    # Reset if state changes
    return Status(ready=False, confidence=0.0)

# Converges when confidence >= threshold
result = await fix(
    transform=poll_with_memory,
    initial=Status(ready=False, confidence=0.0),
    equality_check=lambda prev, curr: curr.ready and curr.confidence >= 0.8
)
```

### Pitfall 4: Implicit Ground Assumptions

**Symptom**: Code assumes platform/environment specifics without declaring them.

**Example**:
```python
# WRONG: Assumes bash shell
subprocess.run("read -p 'Enter name: ' name", shell=True)

# WRONG: Assumes sys.path manipulation works
import sys
sys.path.insert(0, "../../")
from bootstrap import Agent  # Fragile

# WRONG: Assumes current directory
with open("config.json") as f:  # Where is this file?
```

**Fix**: Make Ground explicit in code and documentation.

**Pattern**:
```python
# RIGHT: POSIX-compatible Ground
subprocess.run("printf 'Enter name: ' && read name", shell=True)

# RIGHT: Proper packaging
# pyproject.toml:
# [project]
# dependencies = ["kgents-runtime"]
from bootstrap import Agent

# RIGHT: Explicit paths
from pathlib import Path
config_path = Path(__file__).parent / "config.json"
with open(config_path) as f:
```

**Ground declaration**:
```python
"""
Ground:
- Shell: POSIX-compatible (bash, zsh, sh)
- Python: â‰¥3.11 (match/case, async/await)
- Platform: darwin, linux (no Windows-specific code)
- Packages: kgents-runtime, anthropic, openai
"""
```

### Pitfall 5: Treating Judge as Binary (Ignoring REVISE)

**Symptom**: Only handling ACCEPT or REJECT, missing iterative improvement via REVISE.

**Example**:
```python
# WRONG: Binary judgment
verdict = await judge.invoke(JudgeInput(agent=my_agent, principles=PRINCIPLES))
if verdict.type == VerdictType.ACCEPT:
    print("Done!")
else:
    print("Failed!")  # Gives up on REVISE or REJECT
```

**Fix**: Use REVISE as Fix patternâ€”iterate until ACCEPT.

**Pattern**:
```python
# RIGHT: Iterative improvement via Fix
async def improve_until_accept(agent_code: str) -> str:
    """Apply Judge in Fix patternâ€”iterate until ACCEPT."""
    current = agent_code

    async def judge_and_revise(code: str) -> str:
        verdict = await judge.invoke(JudgeInput(
            agent_spec=code,
            principles=PRINCIPLES
        ))

        if verdict.type == VerdictType.ACCEPT:
            return code  # Converged

        if verdict.type == VerdictType.REVISE:
            # Apply suggested revisions
            revised = apply_revisions(code, verdict.revisions)
            return revised

        if verdict.type == VerdictType.REJECT:
            raise ValueError(f"Judge rejected: {verdict.reasoning}")

    result = await fix(
        transform=judge_and_revise,
        initial=current,
        equality_check=lambda prev, curr: curr == prev  # Stable
    )
    return result
```

### Pitfall 6: Over-Engineering Before Validation

**Symptom**: Adding abstractions, configurability, and "future-proofing" before validating core concept.

**Example**:
```python
# WRONG: Premature abstraction
class ConfigurableAgent:
    def __init__(
        self,
        strategy: Strategy,
        cache: Optional[Cache] = None,
        retry_policy: RetryPolicy = DefaultRetry(),
        observability: ObservabilityHooks = NoOp(),
        feature_flags: Dict[str, bool] = None,
    ):
        # 50 lines of configuration logic before ANY core functionality
```

**Fix**: Implement minimal version first, validate with Judge, THEN generalize if needed.

**Pattern**:
```python
# RIGHT: Start minimal
class MyAgent(Agent[A, B]):
    async def invoke(self, input: A) -> B:
        # Core logic onlyâ€”20 lines
        return result

# Validate first
verdict = await judge.invoke(JudgeInput(agent=my_agent, principles=PRINCIPLES))

# ONLY IF ACCEPT and Kent requests it, add configurability
```

### Pitfall 7: Ignoring Contradict Output

**Symptom**: Running Contradict but not acting on detected tensions.

**Example**:
```python
# WRONG: Detect but ignore
tension = await contradict.invoke((spec, impl))
if tension:
    print(f"Found tension: {tension.description}")
    # Continue anyway without resolution
```

**Fix**: Either Sublate or consciously Hold every tension.

**Pattern**:
```python
# RIGHT: Act on tensions
tension = await contradict.invoke((spec, impl))
if tension:
    resolution = await sublate.invoke(tension)

    if isinstance(resolution, Synthesis):
        # Apply synthesis
        impl = resolution.result
    elif isinstance(resolution, HoldTension):
        # Document and track
        self.log.warning(f"Holding tension: {resolution.why_held}")
        self.held_tensions.append(resolution)
```

### Pitfall 8: Breaking Composition Laws

**Symptom**: Custom composition operators that violate associativity or identity laws.

**Example**:
```python
# WRONG: Non-associative composition
class MyAgent(Agent[A, B]):
    def __rshift__(self, other):
        # Special case that breaks (f >> g) >> h == f >> (g >> h)
        if isinstance(other, SpecialAgent):
            return CombinedAgent(self, other, magic=True)
        return super().__rshift__(other)
```

**Fix**: Preserve category laws. Test associativity and identity explicitly.

**Pattern**:
```python
# RIGHT: Law-preserving composition
class MyAgent(Agent[A, B]):
    def __rshift__(self, other: Agent[B, C]) -> Agent[A, C]:
        # Standard composition, no special cases
        return ComposedAgent(self, other)

# Test laws
def test_composition_laws():
    f, g, h = AgentF(), AgentG(), AgentH()
    assert (f >> g) >> h â‰… f >> (g >> h)  # Associativity
    assert Id() >> f â‰… f  # Left identity
    assert f >> Id() â‰… f  # Right identity
```

### Summary: Pitfall Checklist

Before committing code, check:

- [ ] Did I use agents (K-gent, CreativityCoach, etc.) for non-trivial decisions? (Autopoiesis)
- [ ] Are tensions either Sublated or consciously Held? (No ignored Contradict output)
- [ ] Does Fix accumulate state? (Memory between iterations)
- [ ] Is Ground explicit? (No platform/shell assumptions)
- [ ] Do I handle REVISE verdicts? (Judge as Fix pattern)
- [ ] Is this minimal? (No premature abstraction)
- [ ] Do composition laws hold? (Associativity, identity)
- [ ] Are errors surfaced as data? (Conflicts logged, not swallowed)

**Meta-principle**: When in doubt, use agents to check. Run Judge, Contradict, or K-gent (REFLECT mode) on your own code.

---

## Execution Protocol

### For Each Implementation Phase

```
LOOP:
  1. Use kgents agents to explore design (CreativityCoach, K-gent)
  2. Read relevant spec
  3. Implement (mechanically translate, use LLM to generate boilerplate)
  4. Kent reviews (Judge)
  5. IF accept: mark complete, next phase
     ELIF revise: Claude Code modifies
     ELIF reject: discuss, potentially Sublate
```

### Termination (Fix)

Implementation is complete when:
```python
def protocol_complete() -> bool:
    return all([
        spec_exists(f"spec/{letter}-gents/"),
        impl_exists(f"impl/claude/agents/{letter}/"),
        Judge(impl, Principles).verdict == "accept",
        Contradict(impl, spec) is None,
        autopoiesis_score() > 0.5,
        kent_approves(),
    ])
```

---

## Meta-Instructions for Claude Code

When reading this document:

1. **Instantiate runtime FIRST** â€” Before any implementation
2. **Use K-gent for naming** â€” Don't guess, ask
3. **Use CreativityCoach for design** â€” Explore before coding
4. **Read specs completely** â€” Understand before implementing
5. **Apply the 7 patterns** â€” Events, Ground, Fix memory, Sublate merge, Conflicts as data
6. **Measure autopoiesis** â€” Track what kgents generated vs. manual

### On Tensions

If you encounter contradictions:
1. Surface them explicitly (Contradict)
2. Propose synthesis or hold (Sublate)
3. Let Kent decide (Judge)

### On Uncertainty

If spec is unclear:
1. Use HypothesisEngine to generate possibilities
2. Ask Kent for clarification
3. Propose and iterate (Fix)

---

## Quick Start

```bash
# Begin new implementation:
claude "Read AUTONOMOUS_BOOTSTRAP_PROTOCOL.md. I want to implement {X}. Start by using CreativityCoach to explore the design space."
```

---

## Observability: Making the Protocol Visible

The protocol is only useful if you can observe it working. This section describes how to track protocol execution and measure its health.

### What to Observe

**Core Metrics:**

| Metric | What It Measures | Target | How to Track |
|--------|------------------|--------|--------------|
| **Autopoiesis %** | (Agent-generated lines) / (Total lines) | >50% | Narrative: "Which decisions used agents?" (vibes) |
| **Agent Usage Frequency** | How often agents invoked per phase | Continuous | Count agent calls per implementation phase |
| **Tension Resolution Rate** | Tensions resolved / Tensions detected | >80% | Track Contradict â†’ Sublate outcomes |
| **Judge Acceptance Rate** | ACCEPT / (ACCEPT + REVISE + REJECT) | >60% first pass, 100% after Fix | Track Judge verdicts over time |
| **Fix Convergence** | Iterations to converge | <10 typical | Log Fix iterations, track average |
| **Pattern Adherence** | Required patterns used correctly | 100% | Manual review or linter |

**Qualitative Indicators:**
- Does the process *feel* agent-driven? (Subjective but important)
- Are decisions documented with agent rationale?
- Can you reconstruct *why* a choice was made by reading agent outputs?

### How to Track Protocol Events

**Simple logging pattern:**

```python
"""Protocol Event Logger - Track bootstrap protocol execution"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Literal, Optional
from enum import Enum

class EventType(Enum):
    AGENT_INVOKED = "agent_invoked"
    TENSION_DETECTED = "tension_detected"
    TENSION_RESOLVED = "tension_resolved"
    JUDGE_VERDICT = "judge_verdict"
    FIX_ITERATION = "fix_iteration"
    PATTERN_APPLIED = "pattern_applied"

@dataclass
class ProtocolEvent:
    """Single protocol execution event"""
    type: EventType
    timestamp: datetime = field(default_factory=datetime.now)
    agent_name: Optional[str] = None
    phase: Optional[str] = None  # e.g., "design", "implementation", "validation"
    details: dict = field(default_factory=dict)

class ProtocolObserver:
    """
    Observability layer for AUTONOMOUS_BOOTSTRAP_PROTOCOL execution.

    Usage:
        observer = ProtocolObserver()
        observer.agent_invoked("K-gent", phase="naming")
        observer.judge_verdict(verdict_type="ACCEPT")
        observer.report()  # Print summary
    """

    def __init__(self):
        self.events: list[ProtocolEvent] = []
        self.start_time = datetime.now()

    def agent_invoked(
        self,
        agent_name: str,
        phase: str,
        mode: Optional[str] = None,
        purpose: Optional[str] = None
    ):
        """Record agent invocation"""
        self.events.append(ProtocolEvent(
            type=EventType.AGENT_INVOKED,
            agent_name=agent_name,
            phase=phase,
            details={"mode": mode, "purpose": purpose}
        ))

    def tension_detected(self, tension_type: str, description: str, phase: str):
        """Record Contradict finding tension"""
        self.events.append(ProtocolEvent(
            type=EventType.TENSION_DETECTED,
            phase=phase,
            details={"tension_type": tension_type, "description": description}
        ))

    def tension_resolved(
        self,
        resolution_type: Literal["synthesis", "hold"],
        phase: str
    ):
        """Record Sublate resolution"""
        self.events.append(ProtocolEvent(
            type=EventType.TENSION_RESOLVED,
            phase=phase,
            details={"resolution": resolution_type}
        ))

    def judge_verdict(
        self,
        verdict_type: Literal["ACCEPT", "REVISE", "REJECT"],
        phase: str,
        agent_name: Optional[str] = None
    ):
        """Record Judge verdict"""
        self.events.append(ProtocolEvent(
            type=EventType.JUDGE_VERDICT,
            agent_name=agent_name,
            phase=phase,
            details={"verdict": verdict_type}
        ))

    def fix_iteration(self, iteration: int, phase: str, converged: bool = False):
        """Record Fix iteration"""
        self.events.append(ProtocolEvent(
            type=EventType.FIX_ITERATION,
            phase=phase,
            details={"iteration": iteration, "converged": converged}
        ))

    def pattern_applied(self, pattern_name: str, phase: str):
        """Record required pattern usage"""
        self.events.append(ProtocolEvent(
            type=EventType.PATTERN_APPLIED,
            phase=phase,
            details={"pattern": pattern_name}
        ))

    def report(self) -> str:
        """Generate human-readable protocol execution report"""
        duration = datetime.now() - self.start_time

        # Count by type
        by_type = {}
        for event in self.events:
            by_type[event.type] = by_type.get(event.type, 0) + 1

        # Agent usage narrative
        agents_used = {}
        for event in self.events:
            if event.type == EventType.AGENT_INVOKED and event.agent_name:
                agents_used[event.agent_name] = agents_used.get(event.agent_name, 0) + 1

        # Tension resolution rate
        tensions_detected = by_type.get(EventType.TENSION_DETECTED, 0)
        tensions_resolved = by_type.get(EventType.TENSION_RESOLVED, 0)
        resolution_rate = (
            tensions_resolved / tensions_detected if tensions_detected > 0 else 1.0
        )

        # Judge acceptance (first pass)
        judge_events = [e for e in self.events if e.type == EventType.JUDGE_VERDICT]
        accepts = sum(1 for e in judge_events if e.details.get("verdict") == "ACCEPT")
        acceptance_rate = accepts / len(judge_events) if judge_events else 0.0

        # Format report
        report = f"""
=== AUTONOMOUS BOOTSTRAP PROTOCOL EXECUTION REPORT ===

Duration: {duration.total_seconds():.1f}s

Agent Usage (Autopoiesis Narrative):
"""
        for agent, count in sorted(agents_used.items(), key=lambda x: -x[1]):
            report += f"  - {agent}: {count}x\n"

        report += f"""
Protocol Metrics:
  - Total Events: {len(self.events)}
  - Agent Invocations: {by_type.get(EventType.AGENT_INVOKED, 0)}
  - Tensions Detected: {tensions_detected}
  - Tensions Resolved: {tensions_resolved} ({resolution_rate:.0%} resolution rate)
  - Judge Verdicts: {len(judge_events)} ({acceptance_rate:.0%} acceptance rate)
  - Fix Iterations: {by_type.get(EventType.FIX_ITERATION, 0)}
  - Patterns Applied: {by_type.get(EventType.PATTERN_APPLIED, 0)}

Autopoiesis Assessment:
  {'âœ“ Continuous agent usage' if len(agents_used) >= 3 else 'âš  Low agent diversity'}
  {'âœ“ Healthy tension resolution' if resolution_rate >= 0.8 else 'âš  Many unresolved tensions'}
  {'âœ“ Judge approves work' if acceptance_rate >= 0.6 else 'âš  High rejection rate'}

=== END REPORT ===
"""
        return report

    def autopoiesis_narrative(self) -> str:
        """Generate qualitative autopoiesis story"""
        agent_events = [
            e for e in self.events
            if e.type == EventType.AGENT_INVOKED
        ]

        if not agent_events:
            return "No agent usage detected (0% autopoiesis)"

        narrative = "Autopoiesis Story:\n"
        for event in agent_events:
            phase = event.phase or "unknown"
            agent = event.agent_name or "unknown"
            purpose = event.details.get("purpose", "")
            narrative += f"  - Phase '{phase}': Used {agent}"
            if purpose:
                narrative += f" for {purpose}"
            narrative += "\n"

        return narrative
```

**Usage in practice:**

```python
# Initialize observer at start of implementation session
observer = ProtocolObserver()

# Throughout implementation, log events
observer.agent_invoked("CreativityCoach", phase="design", purpose="explore architecture")
observer.agent_invoked("K-gent", phase="naming", mode="ADVISE")
observer.judge_verdict("REVISE", phase="validation", agent_name="MyNewAgent")
observer.fix_iteration(iteration=1, phase="refinement")
observer.judge_verdict("ACCEPT", phase="validation", agent_name="MyNewAgent")

# At end of session
print(observer.report())
print(observer.autopoiesis_narrative())
```

### Health Dashboard (Conceptual)

Imagine a dashboard showing:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BOOTSTRAP PROTOCOL HEALTH              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Autopoiesis: ðŸŸ¢ 65% (Continuous usage)â”‚
â”‚  Tension Resolution: ðŸŸ¢ 87%            â”‚
â”‚  Judge Acceptance: ðŸŸ¡ 55% (improvable)â”‚
â”‚  Fix Convergence: ðŸŸ¢ 4.2 avg iterationsâ”‚
â”‚  Pattern Adherence: ðŸŸ¢ 100%            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Recent Agent Usage:                    â”‚
â”‚    K-gent: 12x (naming, reflection)    â”‚
â”‚    CreativityCoach: 8x (design)        â”‚
â”‚    HypothesisEngine: 5x (validation)   â”‚
â”‚    Judge: 15x (quality checks)         â”‚
â”‚    Contradict: 7x (consistency)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Key indicators:
- ðŸŸ¢ Green: Meeting target
- ðŸŸ¡ Yellow: Below target but acceptable
- ðŸ”´ Red: Needs attention

### When to Check Observability

**During implementation:**
- After each phase (design, naming, implementation, validation)
- When stuck or uncertain (use metrics to diagnose)
- Before committing code (check autopoiesis %, pattern adherence)

**After implementation:**
- Compare autopoiesis % to target (>50%)
- Review tension resolution rate (>80%)
- Check Judge acceptance trajectory (improving over Fix iterations?)
- Identify which agents were underused

**For retrospectives:**
- Which phases had highest agent usage? (Good!)
- Which phases had low usage? (Opportunity for improvement)
- Did Fix converge quickly? (Sign of good design)
- Were tensions resolved or held? (Held tensions = future work)

### Integrating with Existing Tools

**Git commit messages:**
```bash
git commit -m "feat: Implement MemoryAgent

Autopoiesis: 68% (K-gent 4x, CreativityCoach 3x, Judge 5x)
Patterns: Fix memory, Sublate merge, Events as traces
Judge: ACCEPT after 2 REVISE iterations"
```

**Code comments (optional):**
```python
# Design influenced by CreativityCoach exploration (see session log)
# Named "HistoryAgent" per K-gent recommendation (avoid overloaded "memory")
class HistoryAgent(Agent[Message, List[Message]]):
    ...
```

**Session logs:**
Keep a `session.log` file during implementation:
```
2025-12-08 14:32 - Phase: Design
2025-12-08 14:35 - CreativityCoach invoked: Explored 5 architecture options
2025-12-08 14:42 - HypothesisEngine validated: SQLite approach most suitable
2025-12-08 14:50 - K-gent (ADVISE): Recommended "HistoryAgent" over "MemoryAgent"
2025-12-08 15:10 - Implementation: 120 lines written (guided by design)
2025-12-08 15:25 - Judge verdict: REVISE (needs error handling)
2025-12-08 15:40 - Applied revisions, Judge verdict: ACCEPT
2025-12-08 15:42 - Contradict check: No tensions detected
```

### Observability Anti-Patterns

**Don't:**
- Track metrics mechanically without understanding them (just hitting 50% autopoiesis for its own sake)
- Log every trivial event (noise)
- Optimize metrics at expense of quality (gaming the system)
- Skip observability entirely (flying blind)

**Do:**
- Use metrics as *indicators*, not *goals*
- Tell a coherent narrative of agent usage
- Review observability data during retrospectives
- Adjust when metrics reveal problems

### Meta-Observability

Use agents to observe the protocol itself:

```python
# Use K-gent to reflect on protocol execution
reflection = await runtime.execute(kgent(), DialogueInput(
    message=f"I just implemented X with {autopoiesis_score}% autopoiesis. What could I improve?",
    mode=DialogueMode.REFLECT
))

# Use HypothesisEngine to diagnose low metrics
hypotheses = await runtime.execute(hypothesis_engine(), HypothesisInput(
    observations=[
        f"Autopoiesis only {score}%",
        "Used agents at start, then forgot",
        "Judge rejected 3 times before accepting"
    ],
    domain="software development process",
    question="Why is autopoiesis low and how can I improve?"
))
```

**This is the protocol being self-referential**: Use the bootstrap agents to observe and improve protocol execution itself.

---

## Case Study: Phase 1 Bootstrap Implementation

For a concrete example of this protocol in action, see `impl/claude/IMPLEMENTATION_PLAN.md` which documents the 10 Critical Fixes applied to the bootstrap agents themselves.

**Key observations from that implementation:**
- **Issue #1-2 (Type Fixes)**: Fix and Compose types corrected via systematic type checking â†’ Judge verification â†’ no breaking changes
- **Issue #5 (EvolutionAgent)**: Tension detected (orchestration vs composition) â†’ Sublated into 4 handler classes â†’ 58% code reduction
- **Issue #10 (TensionDetector Protocol)**: Extensibility added via Protocol pattern â†’ composable contradiction detection
- **Autopoiesis**: Used agents (Contradict, Judge, Hegel) to improve agent implementation â†’ self-referential development

**Pattern demonstrated**: The protocol successfully guided refactoring of its own foundation (the bootstrap agents) without breaking composition laws.

---

## Self-Description

This protocol is itself an application of bootstrap agents:

| Protocol Element | Bootstrap Agent |
|------------------|-----------------|
| "Ground" section | Ground |
| Phase ordering | Compose |
| Kent checkpoints | Judge |
| Termination condition | Fix |
| "On Tensions" section | Contradict + Sublate |
| Autopoiesis imperative | Id (self-reference) |

The protocol bootstraps the bootstrap.
